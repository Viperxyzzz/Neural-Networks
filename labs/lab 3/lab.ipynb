{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "x = iris.data[0:100,[2,3]]     # petal length and petal width\n",
    "y = iris.target[0:100]\n",
    "\n",
    "#print('Class labels:', np.unique(y))\n",
    "labels = ['setosa', 'versicolor', 'virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standarization\n",
    "sc = StandardScaler()\n",
    "sc.fit(x)   # to estimate mean and standard deviation\n",
    "xstd = sc.transform(x)\n",
    "x = xstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the one-layer neural network\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "        self.weights = np.random.randn(input_size + 1)\n",
    "\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "      return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "      \n",
    "      self.pred = self.sigmoid(np.dot(x, self.weights[1:]) + self.weights[0])\n",
    "\n",
    "      return self.pred\n",
    "    \n",
    "    def cost_function(self, x , y):\n",
    "        error = y - self.forward(x)\n",
    "        loss = np.mean(error**2)\n",
    "        return loss\n",
    "    \n",
    "    def cost_function_derivative(self, x, y):\n",
    "       pred = self.forward(x)\n",
    "       error = y - pred\n",
    "       bias = -2.0 * np.mean(error * pred * (1 - pred))\n",
    "       weights = -2.0 * np.dot(x.T, error * pred * (1 - pred))\n",
    "       \n",
    "       return [bias, weights]\n",
    "    \n",
    "    def shuffle_data(X, y):\n",
    "        N, _ = X.shape\n",
    "        shuffled_indices = np.random.permutation(N)\n",
    "        return X[shuffled_indices], y[shuffled_indices]\n",
    "    \n",
    "    def stochastic_gradient_descent(self, x, y, learning_rate, epochs):\n",
    "        N = x.shape[0]\n",
    "        J = self.cost_function(x, y)  \n",
    "        for epoch in range(epochs):\n",
    "            for i in range(N):\n",
    "                random_index = np.random.randint(0, N)\n",
    "                x_point = x[random_index]\n",
    "                y_point = y[random_index]\n",
    "                gradients = self.cost_function_derivative(x_point, y_point)\n",
    "                self.weights[0] -= learning_rate * gradients[0]\n",
    "                self.weights[1:] -= learning_rate * gradients[1]\n",
    "\n",
    "#                e = self.cost_function(x, y)\n",
    "\n",
    " #               if abs(J - e) < 0.00001:\n",
    "  #                break\n",
    "\n",
    "   #             J = e\n",
    "        return self.weights\n",
    "    \n",
    "    def batch_gradient_descent(self, x, y, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            gradients = self.cost_function_derivative(x, y)\n",
    "            self.weights[0] -= learning_rate * gradients[0]\n",
    "            self.weights[1:] -= learning_rate * gradients[1]\n",
    "        return self.weights\n",
    "\n",
    "    def minibatch_gradient_descent(self, x, y, learning_rate, epochs, batch_size=3):\n",
    "        N = x.shape[0]\n",
    "        num_batches = N//batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(N)\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            for i in range(num_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = (i + 1) * batch_size\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                x_batch = x[batch_indices]\n",
    "                y_batch = y[batch_indices]\n",
    "\n",
    "                # Calculate gradients and update weights\n",
    "                gradients = self.cost_function_derivative(x_batch, y_batch)\n",
    "                self.weights[0] -= learning_rate * gradients[0]\n",
    "                self.weights[1:] -= learning_rate * gradients[1]\n",
    "            \n",
    "        return \n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            output = self.forward(x)\n",
    "            predictions.append(output)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error = 1.22 %\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = x.shape[1]\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Create and train the neural network\n",
    "model = NeuralNetwork(input_size)\n",
    "#model.minibatch_gradient_descent(x, y, learning_rate, epochs, batch_size=3)\n",
    "model.minibatch_gradient_descent(x, y, learning_rate, epochs)\n",
    "# Make predictions on the entire dataset\n",
    "y_pred = model.predict(x)\n",
    "\n",
    "error = sum(abs(y - y_pred))/len(y)*100\n",
    "print('Error = %2.2f %%'  %error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
